{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S8_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m1qBWqmF1OeC",
        "fBLJAaKpPkqh",
        "AYDgARvHbqB1",
        "OK0nVv1Lox9G",
        "NTcDh7biuXEX",
        "FW4aIk3u1BJP",
        "ml_USXwk4eGH",
        "QJeCwPaTXc-3",
        "3U3vsIZ6XkSt",
        "EjXX04VuBSVs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/iue_curso_ia/blob/main/nb_google_colab/S8_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StfvBy548ac4"
      },
      "source": [
        "  <tr>\n",
        "     <th><p><img alt=\"Colaboratory logo\" height=\"120 px\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/iue.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p></th> \n",
        "     <th><h1>  Algoritmos de DL </h1></th>\n",
        "  </tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qBWqmF1OeC"
      },
      "source": [
        "## **Pipeline**\n",
        "\n",
        "Llamamos pipeline a una secuencia de procesamiento de datos que permite optimizar mi flujo de trabajo. Estos son comunes en machine learning pues no solo permiten la manipulación y transformación de multiples datos, sino que además tenemos una alta reducción en código. Este consiste en encadenar una serie de estimadores, que me permiten ir trasformando un conjunto de datos en un proceso comprendido por varias etapas de manera secuencial, donde cada componente del pipeline toma los datos, los trasforma y su salida va a la siguiente componente del pipeline, dandose esto hasta pasar por todas las componentes de éste. En estos tenemos ciertas ventajas como:\n",
        "\n",
        "* **Selección conjunta de parámetros**: Tenemos facilidad de acceder a los parámetros usados en cada estimador.\n",
        "* **Conveniencia y encapsulación**: Facilidad en la implementación del código, solo es necesario hacer uso del fit y predict, para pasar a través de los estimadores.\n",
        "* **La seguridad**: No se pierde información en los datos que se tiene.\n",
        "\n",
        "En **Sklearn** se cuenta con pipeline, el cual se construye usando parejas de (*key*,*value*), donde **key** será el nombre que le daremos a nuestro paso y **value** el estimador usado, el nombre puede ser cualquiera que deseemos. Todos los estimadores en un pipeline excepto el último deben ser transformadores(es decir deben tener un atributo .transform); el último puede ser de cualquier tipo(clasificación,regresión,transformación,etc.).\n",
        "\n",
        "Veamos un ejemplo sencillo en el cual hacemos uso del **iris data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nusMW--4D4l3"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVjqDSBcEkur"
      },
      "source": [
        "data=load_iris()\n",
        "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7bkGIY4V7a2"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL3NTZysV9pE"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1gBytR4WCLm"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfcCX8HVFQxR"
      },
      "source": [
        "Hasta ahora solo se ha cargado los datos, y los estimadores que se usarán e nuestro proceso. Ahora veamos como se construye el pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKeYAYCJFeD0"
      },
      "source": [
        "model=Pipeline([('scaler',StandardScaler()),('pca',PCA(n_components=2)),('regl',LogisticRegression(random_state=42))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5z811YF_s0"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkfV0i46GCVb"
      },
      "source": [
        "Ya creamos nuestro modelo con ayuda de los pipeline, donde vemos se construyo de la forma en que se especifíco antes, donde tenemos tres estimadores, el primero escala mis variables,el segundo reduce la dimensión y el tercero realiza una regresión logistica. Nosotro podemos usar **make_pipeline**, el cual es una forma rápida de crear nuestro pipeline, pues no debemos poner el nombre de nuestro paso en él."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alvD6uI6GAxH"
      },
      "source": [
        "model_2=make_pipeline(StandardScaler(),PCA(n_components=2),(LogisticRegression(random_state=42)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9RlKaiSId2x"
      },
      "source": [
        "model_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-1OUkrYIhMU"
      },
      "source": [
        "Donde se puede ver que haciendo uso de esta segunda forma para construir el pipeline, se asigna un nombre de forma automática a cada uno de los estimadores, en general es el nombre de este último en minúsculas. Podemos acceder a los estimadores de la siguiente forma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDusbqvI3pr"
      },
      "source": [
        "model.steps[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqZbX3j0NC_1"
      },
      "source": [
        "model[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omUn5YLMNWmL"
      },
      "source": [
        "model['regl']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSlIrlOvNi3q"
      },
      "source": [
        "model.set_params() #parámetros dentro de nuestro pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3cW3zbEN6ch"
      },
      "source": [
        "Ahora bien podemos entrenar, predecir y evaluar nuestros datos, en este caso el iris dataset, con la ayuda del pipeline que construímos haciendo uso de los siguientes atributos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDV0cmkIOLs6"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp2YtuP6ObFV"
      },
      "source": [
        "y_h=model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRa2dWP4OkJy"
      },
      "source": [
        "model.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBLJAaKpPkqh"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Con ayuda del **pipeline** de sklearn, construir uno de forma tal qué, escale mis datos, reduzca mi dimensión(dimensión 2) y aplique un clasificador de arbol de decisión (DecisionTreeClassifier) al iris data set, donde a este último solo le pondremos como parámetro el **random_state=42**. Entrenarlo y evaluarlo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft-GeCI4Q2Yk"
      },
      "source": [
        "Haga click **aquí** si tiene problemas con la solución:\n",
        "\n",
        "<!-----\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_3=Pipeline([('scaler',StandardScaler()),('pca',PCA(n_components=2)),('des_tree',DecisionTreeClassifier(random_state=42))])\n",
        "\n",
        "\n",
        "1.   Elemento de la lista\n",
        "2.   Elemento de la lista\n",
        "\n",
        "\n",
        "model_3.fit(X_train,y_train)\n",
        "model_3.score(X_test,y_test)\n",
        "----->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYDgARvHbqB1"
      },
      "source": [
        "## **Protocolo de validación**\n",
        "\n",
        "Nuestro objetivo es tener modelos que nos permitan generalizar bien, esto es, que tengamos un buen comportamiento a la hora de predecir cuando tengamos nuevos datos, es decir, evitando el overfitting. En busqueda de esto, una parte principal es hacer una correcta elección del modelo y de los hiperparámetros, para esto es necesario tener una forma de **validar** que nuestro modelo e hiperparámetros están adapatandose bien a nuestros datos. Para esto tenemos ciertos protocolos que nos permiten realizar esto:\n",
        "\n",
        "* Simple Hold-out validation\n",
        "* k-fold cross-validation\n",
        "* Iterated k-fold validation  with shuffling\n",
        "\n",
        "#### **Simple Hold-out validation**\n",
        "Basicamente consiste en dividir nuestros datos totales en dos conjuntos, uno el cual llamaremos conjunto de prueba (test set) y es una pequeña fracción de los datos totales; los datos restantes son los que me permitirán entrenar el modelo y es llamado nuestro conjunto de entrenamiento (train set).\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"200px\" src=\"https://i.imgur.com/A6Gzf6M.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        " Veamos como se ve el hold-out validatión en acción, para esto usaremos el iris dataset y un k-neighbors clasiffier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQjAA78Rsnpz"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLjSDZVltSca"
      },
      "source": [
        "iris=load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMc4v_rUtYvQ"
      },
      "source": [
        "X=iris.data\n",
        "y=iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztd3Szlctl0-"
      },
      "source": [
        "X1,X2,y1,y2=train_test_split(X,y,random_state=0,train_size=0.5) #Definimos el 50% de los datos para el conjunto de entrenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4StwQErGt3Qe"
      },
      "source": [
        "model=KNeighborsClassifier(n_neighbors=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEnkK1vCuBE3"
      },
      "source": [
        "model.fit(X1,y1)\n",
        "y2_h=model.predict(X2)\n",
        "accuracy_score(y2,y2_h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVCDOMKCuU8N"
      },
      "source": [
        "En lo anterior vemos que es básicamente lo que hacemos con nuestros modelos de forma usual. Acá tenemos una desventaja, es que parte de nuestros datos no son empleados a la hora de entrenar nuestro modelo, esto hace que perdamos información. Para evitar este tipo de cosas se emplea el siguiente protocolo.\n",
        "\n",
        "#### **k-fold cross-validation**\n",
        "\n",
        "Este conciste en dividir nuestros datos en **k** particiones de igual tamaño, donde cada una de las particiones se usará en entrenamiento y testeo de la siguiente manera, para cada partición \"$i$\", entrenamos el modelo con las restantes **k-1** particiones y lo evaluamos en la partición \"$i$\". El resultado final será el promedio de los **K** resultados obtenidos.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"200px\" src=\"https://i.imgur.com/8TV1oeN.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "Veamos como sería la implementación para el caso de 2-fold cross validation, es decir donde tenemos dos particiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyc8Vwgugsi"
      },
      "source": [
        "y2_model=model.fit(X1,y1).predict(X2)\n",
        "y1_model=model.fit(X2,y2).predict(X1)\n",
        "score=accuracy_score(y1,y1_model),accuracy_score(y2,y2_model)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K_ift-hxj76"
      },
      "source": [
        "import numpy as np\n",
        "np.mean(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKNp8AKZyLSg"
      },
      "source": [
        "La anterior sería una medida global de la realización del modelo. Si bien es facil de realizar para el caso antes visto, cuando tenemos un valor de **k** muy grande hacerlo de forma manual puede ser tedioso, sklearn tiene una rutina que nos permite hacerlo para diferentes **k**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW2fsRLUyuFA"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq0HEyNBzeyW"
      },
      "source": [
        "score=cross_val_score(model,X,y,cv=5)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBvSY184zlun"
      },
      "source": [
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtL86WRg0x8F"
      },
      "source": [
        "Cuando aplicamos k-fold cross-validation multiples vecez, mezclando los datos antes de hacer la partición cada vez, es un protocolo conocido como **Iterated k-fold validation with shuffling**.\n",
        "\n",
        "### **Ejercicio**\n",
        "\n",
        "Realizar un **pipeline** en el cual se **escalen** mis datos del iris data set y luego se use el clasificador **k-neighbors clasiffier** para **n_neighbors=1**. Finalmente usar en el anterior el protocolo de **k-fold cross-validation** para k=5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hYygKEQ3pMO"
      },
      "source": [
        "Haga click **aquí** si tiene problemas con la solución:\n",
        "\n",
        "<!-----\n",
        "pip=Pipeline([('stan',StandardScaler()),('kneig',KNeighborsClassifier(n_neighbors=1))])\n",
        "cross_val_score(pip,X,y,cv=5)\n",
        "----->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNIpiDym7ZBx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK0nVv1Lox9G"
      },
      "source": [
        "# Reto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Ltj9vTpUDF"
      },
      "source": [
        "Haga una red neuronal de 2 capas (2 neuronas ocultas y 1 de salida) usando únicamente numpy para ajustar el XOR usando la función escalera como función de activación. Tip: XOR=AND(OR,NAND)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nor9bl9eqbQ8"
      },
      "source": [
        "Haga click **aquí** si tiene problemas con la solución:\n",
        "\n",
        "<!-----\n",
        "import numpy as np\n",
        "Al=[0]*2\n",
        "Bl=[0]*2\n",
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Al[0]=np.array([[1/2,-1/3],[1/2,-1/3]])\n",
        "Bl[0]=np.array([-1/2,1/2])\n",
        "Al[1]=np.array([[1/3],[1/3]])\n",
        "Bl[1]=np.array([-1/2])\n",
        "step=lambda x:(x>=0)\n",
        "Xl=[]\n",
        "x=X\n",
        "for L in range(2):\n",
        "  x=step(np.dot(x,Al[L])+Bl[L])\n",
        "  Xl.append(x)\n",
        "print(Xl)\n",
        "----->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dF9kV6opIRi"
      },
      "source": [
        "import numpy as np\n",
        "Al=[0]*2\n",
        "Bl=[0]*2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEfcoJOOeFHV"
      },
      "source": [
        "Bl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co_8mSKseK8i"
      },
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlVdsRuQd8MQ"
      },
      "source": [
        "Al[0]=np.array([[1/2,-1/3],[1/2,-1/3]])\n",
        "Bl[0]=np.array([-1/2,1/2])\n",
        "Al[1]=np.array([[1/3],[1/3]])\n",
        "Bl[1]=np.array([-1/2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fAfcp6leSS9"
      },
      "source": [
        "Bl[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEkP7Lj6d-r-"
      },
      "source": [
        "step=lambda x:(x>=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_i0rcebgLN9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28xP08qae7We"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h1rsCFRfBbx"
      },
      "source": [
        "plt.plot(np.arange(-10,10),step(np.arange(-10,10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBAkLCmIeAAm"
      },
      "source": [
        "Xl=[]\n",
        "x=X\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8tSplEfSGe"
      },
      "source": [
        "for L in range(2):\n",
        "  x=step(np.dot(x,Al[L])+Bl[L])\n",
        "  Xl.append(x)\n",
        "print(Xl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBjwUOyOgX9q"
      },
      "source": [
        "Xoculta=step(np.dot(X,Al[0])+Bl[0])\n",
        "Xoculta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8t9ZPvlgX1l"
      },
      "source": [
        "Xfinal=step(np.dot(Xoculta,Al[1])+Bl[1])\n",
        "Xfinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuuu7FGqqs4T"
      },
      "source": [
        "Ahora entrene una red neuronal usando Keras con la misma arquitectura pero usando función de activación ReLu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej6_Pdi5qz2f"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXx5h-J7hneC"
      },
      "source": [
        "modelo=Sequential([Dense(2,input_shape=(2,),activation='tanh'),Dense(1,activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYJgZ3jHiXmU"
      },
      "source": [
        "modelo.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU2KMHpNiwAF"
      },
      "source": [
        "modelo.compile(optimizer='sgd',loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_ZZn-csjMzO"
      },
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y=np.array([0,1,1,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJybAzgajKv3"
      },
      "source": [
        "modelo.fit(x=X,y=y,epochs=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C465uhl3jzx6"
      },
      "source": [
        "modelo.predict_classes(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erS3n78auMP6"
      },
      "source": [
        "# Arquitectura y funcionalidad de la Redes neuronales secuenciales:\n",
        "\n",
        "De las funciones de activación habladas en la clase anterior podemos advertir dos características que deben poseer una red neuronal:\n",
        "\n",
        "1. Las funciones de activación de las capas ocultas deben ser funciones de activación no lineales, con el fín de que la red actue como un **aproximador universal a una función**.\n",
        "2. La función de activación de la capa de salida determina el tipo de clasificación/regresión del problema que se pretende solucionar.\n",
        "\n",
        "Como regla general, se tiene que la función de activación de las capas ocultas puede ser definida como una función `ReLU` y, dependiendo del problema, podemos definir la función de activación de la capa de salida como:\n",
        "\n",
        "* Función de activación sigmoide: si el problema de clasificación es binario.\n",
        "* Función de activación Softmax: si el problema de clasificación es multiclase.\n",
        "* Finción de activación lineal: si el problema se trata de una regresión.\n",
        "\n",
        "En resumen, en la siguiente figura se ilustran la arquitectura de red de los problemas que pueden presentarse en la clasificación/regresión usando una red neuronal secuencial y las funciones de activación definidas en las capas que la componen.\n",
        "\n",
        "<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/archi_clas_reg.png?raw=true\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "Veamos a continuación, someramente y sin entrar en detalles, los tres problemas de clasificación/regresión que pueden abordarse, a saber, la clasificación binaria, la clasificación multiclase y la regresión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTcDh7biuXEX"
      },
      "source": [
        "# Clasificación Binaria:\n",
        "\n",
        "**Objetivo:** Clasificación de reseñas de películas.\n",
        "\n",
        "**Input:** Reseñas\n",
        "\n",
        "**Output:** positiva o negativa.\n",
        "\n",
        "**Base de datos:** Se usará un dataset de IMDB (Internet Movie Databased) el cual consta de 50000 reseñas altamente polarizadas. Estas reseñas se dividen en un conjunto de 25000 reseñas para entrenamiento  y 25000 reseñas de evaluación, cada una de las cuales consta de un 50% de reseñas positivas y un 50% de reseñas negativas. \n",
        "\n",
        "El dataset de IMBD se encuentra Keras. Este dataset ha sido previamente procesado, en ddonde, las reseñas (secuencia de palabras) ha sido transformada en una secuencia de enteros, en donde cada entero corresponde a una palabra específica en un diccionario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkHaibTQuP5L"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtDHkPNh08AY"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jlry_6QuqFJ"
      },
      "source": [
        "A continuación, se realiza un tratamiento de los datos con el fin de convertirlos a vectores de `numpy` que puedan ser usados en una red neuronal. Para mayor información de este paso, consultar el libro del creador de `keras` [Deep learning with python](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYuOT4O1ugOO"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000): \n",
        "    results = np.zeros((len(sequences), dimension)) # Creates an all-zero matrix of shape (len(sequences), dimension)\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # Sets specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data) # Vectorized training data\n",
        "x_test = vectorize_sequences(test_data) # Vectorized training data\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32') # Vectorized training labels. \n",
        "y_test = np.asarray(test_labels).astype('float32') # Vectorized test labels."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkqpMigU2Fbi"
      },
      "source": [
        "len(train_data[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6JSGoWn1j9A"
      },
      "source": [
        "x_train[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8Tq5CIwY1C"
      },
      "source": [
        "Definamos un modelo de dos capas ocultas, con funciones de activacióón **ReLU**, y una capa de salida con una función de activación **Sigmoide**, con el fin de abordar el problema de clasificación binario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M6rYWBRve03"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBP06EgQ2gC6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bscfzlH2flX"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=4, batch_size=512,validation_data=(x_test,y_test))\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W996ZjeJwuEo"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iPbKs0txGFe"
      },
      "source": [
        "# Predicción del modelo:\n",
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdeGGFQZ3JR_"
      },
      "source": [
        "y_preds=model.predict_classes(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJBmx733W0L"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKhMgCNA3a5A"
      },
      "source": [
        "confusion_matrix(y_test,y_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW4aIk3u1BJP"
      },
      "source": [
        "# Clasificación Multiclase:\n",
        "\n",
        "**Objetivo:** Clasificación de un cable de noticias de un dataset de *Reuters*.\n",
        "\n",
        "**Input:** Noticias.\n",
        "\n",
        "**Output:** 46 topicos diferentes.\n",
        "\n",
        "**Base de datos:** Cada topido tiene al menos 10 ejemplos en el set de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAOSXbMeybJp"
      },
      "source": [
        "from keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9DOYwoI1OtY"
      },
      "source": [
        "A continuación, se realiza un tratamiento de los datos con el fin de convertirlos a vectores de `numpy` que puedan ser usados en una red neuronal. Para mayor información de este paso, consultar el libro del creador de `keras` [Deep learning with python](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdAikHZa1Ilm"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    \n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46): \n",
        "    results = np.zeros((len(labels), dimension))\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "        \n",
        "    return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "\n",
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bEx5IpI8ndX"
      },
      "source": [
        "train_labels[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6WukXfE8rNL"
      },
      "source": [
        "one_hot_train_labels[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2bCIqQV2swM"
      },
      "source": [
        "Creemos a continuación una red neuronal con dos capas ocultas (de 64 neuroranas cada una), con funciones de activación **ReLU**, y una capa de salida con una función de activación **softmax**, con 46 neuronas, una por cada tópico:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2X93NrX1cWM"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFJyfzVV8741"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRLTgU9I9IN2"
      },
      "source": [
        "model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TOSGUld9OAp"
      },
      "source": [
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDxFxYg3glK"
      },
      "source": [
        "Veamos los resultados de la evaluación del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhio9Ee10Xb"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks5ZRXK03jes"
      },
      "source": [
        "Y procesamos a entender los resultados de la prediccióón para una de las instancias del conjunto de evaluación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcHOlt0v2M3f"
      },
      "source": [
        "# predicción.\n",
        "predictions = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16itsE49XEZ"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yt9-yBt9c9t"
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePLNbcPt33N0"
      },
      "source": [
        "Para la predicción de la instancia 0, se tiene que el vector de salida tiene la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA-6wj1r2bBw"
      },
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4HsY5RH4Edn"
      },
      "source": [
        "El vector tiene 46 entradas, que corresponden a las probabilidades de pertenencia a cada una de las 46 clases. La suma ade ellas debe ser uno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbdc6ctF2hXj"
      },
      "source": [
        "np.sum(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1i1vJGN4QI7"
      },
      "source": [
        "La clase más probable a la cual pertenece esta instancia es la clase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Xu1GnR2kib"
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neMi3MIa9102"
      },
      "source": [
        "class_pred=model.predict_classes(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln4JudK597D3"
      },
      "source": [
        "class_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKKamRJJ99V8"
      },
      "source": [
        "class_pred[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml_USXwk4eGH"
      },
      "source": [
        "# Regresión: Predicción de los precios de una casa:\n",
        "\n",
        "Usaremos el dataset de los precios de casas en Bostón, para realizar la predicción.\n",
        "\n",
        "El objetivo es obtener a la salida de la red neuronal un valor numérico del precio predicho en función de las caracteristicas de la tabla.\n",
        "\n",
        "Definiremos un arquitectura de red con dos capas ocultas de 64 neuronas, usando una funcióón de activación **ReLu**, en cada una, y una capa de salida de una neurona, con una función de **activación lineal**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhVvfpZz2mcI"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "# normalización de los datos\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std\n",
        "\n",
        "#Función para la definición del modelo\n",
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model(): \n",
        "  model = models.Sequential() \n",
        "  model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1)) #Función de activación lineal\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfEs3Pr3xvKp"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atBp6RMKxRHS"
      },
      "source": [
        "# Creación del modelo\n",
        "modelo = build_model()\n",
        "modelo.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAgxqXZCxSxI"
      },
      "source": [
        "modelo.fit(train_data, train_targets, epochs=500, batch_size=16,validation_data=(test_data,test_targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHezKotxqRZ"
      },
      "source": [
        "test_mse_score, test_mae_score = modelo.evaluate(test_data, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFhl7cTE5K3A"
      },
      "source": [
        "modelo.evaluate(train_data,train_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evdyO4xM7Bax"
      },
      "source": [
        "modelo.evaluate(test_data,test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJeCwPaTXc-3"
      },
      "source": [
        "# Ejemplo 1: Calssificacion binaria \n",
        "objetivo : predecir si las personas tienen o no una enfermedad del corazon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFHfVh6j6P2y"
      },
      "source": [
        "Importmeos algunas de las librerias que nos seran de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K226rJc6P21"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIfQBW5LeBsR"
      },
      "source": [
        "Primero importemos el dataset que se encuentran en Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4itC9URSEWt"
      },
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/diplomado-bigdata-machinelearning-udea/Curso2/master/Datasets/heart.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_weWrM7bejEI"
      },
      "source": [
        "Inspeccionemos el dataset y sus características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v76ITKBLSN9V"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILyeKDAmo9n1"
      },
      "source": [
        "Context\n",
        "This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n",
        "\n",
        "Content\n",
        "\n",
        "Attribute Information:\n",
        "It's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n",
        "\n",
        "1. age: The person's age in years\n",
        "\n",
        "2. sex: The person's sex (1 = male, 0 = female)\n",
        "\n",
        "3. cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n",
        "\n",
        "4. trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n",
        "\n",
        "5. chol: The person's cholesterol measurement in mg/dl\n",
        "\n",
        "6. fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
        "\n",
        "7. restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
        "\n",
        "8. thalach: The person's maximum heart rate achieved\n",
        "\n",
        "9. exang: Exercise induced angina (1 = yes; 0 = no)\n",
        "\n",
        "10. oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n",
        "\n",
        "11. slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n",
        "\n",
        "12. ca: The number of major vessels (0-3)\n",
        "\n",
        "13. thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n",
        "\n",
        "14. target: Heart disease (0 = no, 1 = yes)\n",
        "\n",
        "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n",
        "\n",
        "To see Test Costs (donated by Peter Turney), please see the folder \"Costs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjHqoCuEfHk5"
      },
      "source": [
        "Renombremos las columnas del dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFoSt8kfS1zG"
      },
      "source": [
        "data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n",
        "       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJdOByAs91U0"
      },
      "source": [
        "Ahora procedamos a analizar un poco el dataset con algunos gráficos simples que nos darán una idea del problema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzo6ZXEcSOi-"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGG7-4pQrU96"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26QQZvkriq9"
      },
      "source": [
        "print(data.target.value_counts())\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x='target', data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiab2M-3csUA"
      },
      "source": [
        "fig, ax = plt.subplots(1,4,figsize=(20,6))\n",
        "sns.boxplot(x='target',y='resting_blood_pressure' ,data=data, ax=ax[0])\n",
        "sns.boxplot(x='target',y='cholesterol' ,data=data,ax=ax[1])\n",
        "sns.boxplot(x='target',y='max_heart_rate_achieved' ,data=data,ax=ax[2])\n",
        "sns.boxplot(x='target',y='st_depression' ,data=data,ax=ax[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atPX81PoOaIc"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', square=True)\n",
        "b, t = plt.ylim() \n",
        "b += 0.5 \n",
        "t -= 0.5 \n",
        "plt.ylim(b, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WVS2U4196a0"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "sns.countplot(x='age', data=data, hue='target')\n",
        "plt.legend([\"Haven't Disease\", \"Have Disease\"],loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9e6PA1tagJe"
      },
      "source": [
        "def plot_count_bar(feature):\n",
        "  fig, ax = plt.subplots(1,2,figsize=(10,7))\n",
        "  sns.countplot(x=feature, data=data, hue='target', ax=ax[0])\n",
        "  ax[0].legend([\"Haven't Disease\", \"Have Disease\"],loc='best')\n",
        "  sns.barplot(x=feature, y='target', data=data,ax=ax[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtGmhP0MK88"
      },
      "source": [
        "data['sex'] = data['sex'].map({0:'female', 1:'male'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXbMCsdZ97xi"
      },
      "source": [
        "plot_count_bar('sex')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5WjocjnN4u0"
      },
      "source": [
        "data['chest_pain_type'] = data['chest_pain_type'].map({0:'typical angina', 1:'atypical angina', 2:'non-anginal pain', 3:'asymptomatic'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SObK2xGzh7f"
      },
      "source": [
        "plot_count_bar('chest_pain_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyucmQI705Vr"
      },
      "source": [
        "data['fasting_blood_sugar'] = data['fasting_blood_sugar'].map({0:'lower than 120mg/ml',1:'greater than 120mg/ml'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTqXU2pU3P4"
      },
      "source": [
        "plot_count_bar('fasting_blood_sugar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tycaUf0TVBs1"
      },
      "source": [
        "data['rest_ecg'] = data['rest_ecg'].map({0:'normal', 1 : 'ST-T abnormality', 2:'hypertrophy'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tm8syHKWOgn"
      },
      "source": [
        "plot_count_bar('rest_ecg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9SudVjPaI_c"
      },
      "source": [
        "data['exercise_induced_angina'] = data['exercise_induced_angina'].map({0:'no', 1 : 'yes'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoMGy3UEeHKO"
      },
      "source": [
        "plot_count_bar('exercise_induced_angina')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ7-g3ugeK1K"
      },
      "source": [
        "data['st_slope'] = data['st_slope'].map({0: 'upsloping',  1: 'flat', 2: 'downsloping'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkbmQ3YmgMdW"
      },
      "source": [
        "plot_count_bar('st_slope')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0MrZmGjhAhH"
      },
      "source": [
        "data['thalassemia'] = data['thalassemia'].map({1 :'normal', 2 : 'fixed defect', 3 :'reversable defect'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TimysrbZgO8W"
      },
      "source": [
        "plot_count_bar('thalassemia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1F4bzPWhZWz"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578jrT7Ii4YS"
      },
      "source": [
        "X=data.iloc[:,:-1]\n",
        "y=data['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dstBkBFbiX6w"
      },
      "source": [
        "X=pd.get_dummies(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfZXWR1Ahsgg"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZx5_oRlifjL"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRcvPGhh2N2"
      },
      "source": [
        "Creemos ahora un set de entrenamieto y otro de testeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFTqv32YilE3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySEspJI7ivBy"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uORS9QXkNoC"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE0e-O7DiEUa"
      },
      "source": [
        "Recordemos como son implementados los pipelines usando sklearn. Para estos utilizareos el modelo de regression logistica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8gUEiI3iAgH"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr-5N-IXiAV4"
      },
      "source": [
        "model = Pipeline((\n",
        "    ('scale', StandardScaler()), ('log_reg',LogisticRegression(C=10, solver='lbfgs', n_jobs=-1, fit_intercept=True))\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbhGHr_iATn"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckyOE8ddiAPN"
      },
      "source": [
        "y_fit=model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksFtjT8iANC"
      },
      "source": [
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJhQyCt7iABh"
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(confusion_matrix(y_test,y_fit), annot=True, square=True)\n",
        "b, t = plt.ylim() \n",
        "b += 0.5 \n",
        "t -= 0.5 \n",
        "plt.ylim(b, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8c5xj3lodt"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "pd.Series((model[1].coef_[0]), index=X_train.columns).sort_values().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwEo6VSApNA6"
      },
      "source": [
        "Recordemos que además la búsqueda de hyperparameters puede automatizarse con sklearn usando `gridsearchcv` o `randomsearchcv`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLxJAnDgpN4r"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCzMo3hQpN1A"
      },
      "source": [
        "grid_params = {\n",
        "    'log_reg__C':np.linspace(1,100,5),\n",
        "    'log_reg__fit_intercept': [True,False]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpRXUxspNyS"
      },
      "source": [
        "grid_result = GridSearchCV(model, grid_params, n_jobs=-1, cv=5, iid=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAgt6MSpOpr"
      },
      "source": [
        "grid_result.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmpJd_8drCSR"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dI_XhDrrmT8"
      },
      "source": [
        "model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugOfPeERrvbM"
      },
      "source": [
        "y_fit=model.predict(X_test)\n",
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO81DakMiNgd"
      },
      "source": [
        "## Ahora usemos keras para crear un modelo de redes neururonales "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K77EJLWth2Ow"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupRLNM6todR"
      },
      "source": [
        "Antes de usar los pipelines en keras recordemso como hemos estado trabajando. primero debemos escalar los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7qgyq2Ws-Tb"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic3I48zJs-H-"
      },
      "source": [
        "scale = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGo6AmDLs98D"
      },
      "source": [
        "X_train_scaled = scale.fit_transform(X_train)\n",
        "X_test_scaled = scale.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck6LBZOqs9uZ"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVEcgXXXuhe-"
      },
      "source": [
        "Ahora construyamos nuestra red neuronal de capas densas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxd3Cjrsmmrr"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "     keras.layers.Dense(5, activation='relu', input_shape = X_train.shape[1:]),    \n",
        "     keras.layers.Dense(5, activation='relu'), \n",
        "     keras.layers.Dense(1 , activation='sigmoid')   \n",
        "                             \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD5-pgrXumCf"
      },
      "source": [
        "pasemos a compilar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdECLLdlmmoV"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9HfzM1uqwo"
      },
      "source": [
        "y finalmente entrenemos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrpsAX2qmmmE"
      },
      "source": [
        "model.fit(X_train_scaled,y_train, epochs=50, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRUJoy3Vu1y-"
      },
      "source": [
        "y_fit=model.predict_classes(X_test_scaled)\n",
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsof8zuop1L-"
      },
      "source": [
        "Ahora usemos keras junto a sklear para automatizar este proceso de escalado y ademas para automatizar la busqueda de hyperparametros como se hizo con la regresion logistica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHTqKaLAmmhg"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIjNrwgnvN_T"
      },
      "source": [
        "El primer paso es crear una funcion que constriuira y compilara el modelo de Keras dado un conjunto de hyperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUH0HCaeiWbQ"
      },
      "source": [
        "def build_model(n_neurons=5,input_shape=(25,)):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_neurons, activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.Dense(n_neurons, activation='relu' ))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfbWrMPtvljH"
      },
      "source": [
        "Ahora creemos un `KerasClassifier` basados en la funcion build_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9h0tnntxzkF"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=100, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0doF9xLv6eU"
      },
      "source": [
        "Ahora podemos pasar a usar este objeto como un modelo usual de clasificacion en Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDwD4jB9rfAG"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4dTlk7vnUM3"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaFlqhj5wWWj"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "279JJXynw0Wx"
      },
      "source": [
        "y_fit=model.predict(X_test)\n",
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDO1DtOBxjQg"
      },
      "source": [
        "Ahora pasemos buscar cual seria el mejor numero de capas ocultas, neuronas por capa oculta y batch_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V01TzY0X4_L4"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckRHEYYR4_MG"
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=5,input_shape=(25,)):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_neurons, activation='relu', input_shape=input_shape))\n",
        "  for i in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFNDr6-i7htY"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=50, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdG2xSSW70Ii"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Gh4UplyI9s"
      },
      "source": [
        "params = {\n",
        "    'ann__n_hidden':[0,1,2,3],\n",
        "    'ann__n_neurons':np.arange(0,15),\n",
        "    'ann__batch_size':[10,15,20]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf8vGBFr6F3J"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZXNOFRy6K5C"
      },
      "source": [
        "rnd_search = RandomizedSearchCV(model, params, n_iter=50, n_jobs=-1, cv=3, scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "846taQpV7Isw"
      },
      "source": [
        "grid_result = rnd_search.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLWkshPb0yzN"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjJ8HLVyjN6g"
      },
      "source": [
        "sk_params = {\n",
        "    'batch_size':grid_result.best_params_['ann__batch_size'],\n",
        "    'n_hidden': grid_result.best_params_['ann__n_hidden'],\n",
        "    'n_neurons':grid_result.best_params_['ann__n_neurons']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYspkoVqjmPt"
      },
      "source": [
        "sk_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3xTxV6CC6K"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O40_zjaMBk1i"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=50, verbose=1, **sk_params )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GLSS75YBk1n"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KXTxsnc2Fpq"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfs5iaEIyY52"
      },
      "source": [
        "y_fit=model.predict(X_test)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_test,y_fit), annot=True)\n",
        "b, t = plt.ylim() \n",
        "b += 0.5 \n",
        "t -= 0.5 \n",
        "plt.ylim(b, t)\n",
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U3vsIZ6XkSt"
      },
      "source": [
        "# Ejemplo 2: clasificacion multicalse \n",
        "Objetivo: predecir entre 6 tipos de vridiros diferentes apartir de su composicion quimica "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy9NI4qk4mr4"
      },
      "source": [
        "Importmeos algunas de las librerias que nos seran de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t03CjxUiqNBS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xV-vrdr4tcA"
      },
      "source": [
        "Imortemos los datos desde el repositorio de github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0ZQ7DDXm_e"
      },
      "source": [
        "data=pd.read_csv('https://raw.githubusercontent.com/diplomado-bigdata-machinelearning-udea/Curso2/master/Datasets/glass.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovJFzkxb6suO"
      },
      "source": [
        "Inspeccionemos el dataset y sus características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0FB4IJoTQfV"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEZHP8aQTRNu"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkWte_NgTpAZ"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhHKMIaWTpAj"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7H623v6u91"
      },
      "source": [
        "Ahora procedamos a analizar un poco el dataset con algunos gráficos simples que nos darán una idea del problema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOLlG_BFcUSV"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', square=True)\n",
        "b, t = plt.ylim() \n",
        "b += 0.5 \n",
        "t -= 0.5 \n",
        "plt.ylim(b, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akAyc24STpAm"
      },
      "source": [
        "print(data.Type.value_counts())\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x='Type', data=data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y7aa4Qqouue"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqjyOpc6n7dH"
      },
      "source": [
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knqOZSdg-G-w"
      },
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(23,8))\n",
        "names=data.columns\n",
        "for i , ax in enumerate(ax.flat):  \n",
        "  sns.boxplot(data=data, x='Type', y=names[i] , orient='vertical' , ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-iaRnpY7Gt5"
      },
      "source": [
        "Como vemos hay varios Outliers. Identifiquemos y removamos estos Outliers usando el metodo del interquartile range (IQR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T19jVzKl4h7R"
      },
      "source": [
        "def IQR(x, value=1.5):\n",
        "  Q1 = np.nanpercentile(x,25)\n",
        "  Q3 = np.nanpercentile(x,75)\n",
        "  IQR = Q3 - Q1\n",
        "  upper = Q3 + value*IQR\n",
        "  lower = Q1 - value*IQR\n",
        "  x[x > upper] = np.nan\n",
        "  x[x < lower] = np.nan\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lh68JzvMqbt"
      },
      "source": [
        "for i in data.columns[:-1]:\n",
        "  for j in data['Type'].unique():\n",
        "    data.loc[data['Type']==j, i]=IQR(data.loc[data['Type']==j, i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iJ3-ZGH7zn8"
      },
      "source": [
        "La estrategia implementada para remplazar estos Outliers sera usar el promedi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VqYMGJGOeXY"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "for i in data['Type'].unique():  \n",
        "  data.loc[data['Type']==i,'RI':'Fe']=imp.fit_transform(data.loc[data['Type']==i,'RI':'Fe'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTzVzVk9NuX4"
      },
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(23,8))\n",
        "names=data.columns\n",
        "for i , ax in enumerate(ax.flat):  \n",
        "  sns.boxplot(data=data, x='Type', y=names[i] , orient='vertical' , ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83RIfAK87-gf"
      },
      "source": [
        "Como es usual separemos nuestro datos en entrenemaiento y testeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX2nZev8rDR3"
      },
      "source": [
        "X=data.iloc[:,:-1]\n",
        "y=data['Type']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovJWoJsMrDSF"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px17xD_lrIJ9"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_EI_KHArDSJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwgcVjhGrDSM"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7eSpnuJrDSO"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al7sVvBn9teB"
      },
      "source": [
        "Ahora procedamos a implementar nuestro modelo de redes neuronales. Usaremos validacion curazada y RandomSearchCv para buscar los hyperparametros de nuetro modelo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3cRmsDHsYoM"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ywUCAdBn5eA"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEL9nPEZn5eI"
      },
      "source": [
        "def build_model(n_neurons=55, n_hidden=2, input_shape=(9,)):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_neurons, activation='relu', input_shape=input_shape))\n",
        "  for i in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
        "  model.add(keras.layers.Dense(6, activation='softmax'))\n",
        "  model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fnu0aoO9Rmp"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5aClZmm9Rmt"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNRUSskK9Rm0"
      },
      "source": [
        "params = {\n",
        "    'ann__n_hidden':[0,1,2,3],\n",
        "    'ann__n_neurons':np.arange(45,60),\n",
        "    'ann__batch_size':[10,15,20]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-SmdC89Rm3"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdU3xWRX9Rm5"
      },
      "source": [
        "rnd_search = RandomizedSearchCV(model, params, n_iter=50, n_jobs=-1, cv=3, scoring='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZBxDjhZ9RnA"
      },
      "source": [
        "grid_result = rnd_search.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UatF81--hyv"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG_PiDM5leFe"
      },
      "source": [
        "sk_params = {\n",
        "    'batch_size':grid_result.best_params_['ann__batch_size'],\n",
        "    'n_hidden': grid_result.best_params_['ann__n_hidden'],\n",
        "    'n_neurons':grid_result.best_params_['ann__n_neurons']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYsFvSq0lgOS"
      },
      "source": [
        "sk_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bcPRUcT-hyy"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmztUswP-hy0"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model, epochs=100 , verbose=1, **sk_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAuYpE5l-hy2"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gzq7VUW-hy7"
      },
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P4PncTF-hy8"
      },
      "source": [
        "y_fit=model.predict(X_test)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_test,y_fit), annot=True)\n",
        "b, t = plt.ylim() \n",
        "b += 0.5 \n",
        "t -= 0.5 \n",
        "plt.ylim(b, t)\n",
        "print(classification_report(y_test,y_fit))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O26S8bN6WGUO"
      },
      "source": [
        "Comparemos los resultados obtneido con un el modelo de regression SoftMax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqNI5l9LKeD"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuOEAq_Lv0_"
      },
      "source": [
        "model = Pipeline((\n",
        "            ('scale', StandardScaler())  , ('log_reg', LogisticRegression( multi_class='multinomial', solver='lbfgs', C=30, max_iter=500, ))\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wJ52J0qB7I1"
      },
      "source": [
        "grid_params= {\n",
        "    'log_reg__C':np.arange(1,50),\n",
        "    'log_reg__class_weight':['balanced', None]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_Mjc8M0SqR"
      },
      "source": [
        "grid_result = GridSearchCV(estimator=model, param_grid=grid_params, n_jobs=-1, cv=5)\n",
        "grid_result.fit(X_train,y_train)\n",
        "print(grid_result.best_params_)\n",
        "model=grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLLMLwBcMCsZ"
      },
      "source": [
        "model.fit(X_train,y_train)\n",
        "y_fit=model.predict(X_test)\n",
        "print(classification_report(y_test,y_fit))\n",
        "sns.heatmap(confusion_matrix(y_test,y_fit), annot=True, cmap='RdYlGn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjXX04VuBSVs"
      },
      "source": [
        "# Ejemplo 3: Regression \n",
        "Objetivo: predecir el precio de casas en la ciuidad de california "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJkZCvWhIGly"
      },
      "source": [
        "importemos algunas de las ligrerais que nos seran de utilidad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ljnEURfctE2"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSOzJEi2SHl1"
      },
      "source": [
        "importemos el dataset usando keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbTCXqmGWnw1"
      },
      "source": [
        "housing = fetch_california_housing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6NvZDRKIf4Y"
      },
      "source": [
        "Inspeccionemos el dataset y sus características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNYiReMHIeaM"
      },
      "source": [
        "housing.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TLAnBU5W3tQ"
      },
      "source": [
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='Price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ES9UXSzNPHs"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffNApYR7IcyS"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMm5S2nFIrlE"
      },
      "source": [
        "print(housing.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mQhLpjaJNCK"
      },
      "source": [
        "plt.figure(figsize=(8,7))\n",
        "sns.heatmap(X.join(y).corr(), square=True, annot=True, cmap='RdYlGn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsaetl3YL2fg"
      },
      "source": [
        "Separemos nuestros datos en un set de entrenamiento y otro de testeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qdT3QmgXLgT"
      },
      "source": [
        "X_train , X_test, y_train, y_test =  train_test_split(X,y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2zFbhR-YU7A"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7uv8RvYMQbd"
      },
      "source": [
        "Ahora procedamos a implementar nuestro modelo de redes neuronales. Usaremos validacion curazada y RandomSearchCv para buscar los hyperparametros de nuetro modelo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq7YyaoqObFj"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_-bjS_ObFs"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWB1t7z8ObFu"
      },
      "source": [
        "def build_model(n_neurons=5, n_hidden=2, input_shape=(8,)):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(n_neurons, activation='relu', input_shape=input_shape))\n",
        "  for i in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1))\n",
        "  model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Yv4lu9ObFv"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasRegressor(build_fn=build_model, epochs=30, verbose=1, validation_split=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_WK4S_kObFz"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sASBOta3ObF2"
      },
      "source": [
        "params = {\n",
        "    'ann__n_hidden':[0,1,2,3],\n",
        "    'ann__n_neurons':np.arange(20,40),\n",
        "    'ann__batch_size':[10,15,20,25]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7_cfwW7ObF5"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLVceqkXObF7"
      },
      "source": [
        "rnd_search = RandomizedSearchCV(model, params, n_iter=10, n_jobs=-1, cv=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRSLXSPObF9"
      },
      "source": [
        "grid_result = rnd_search.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU-WeT6fObF_"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A-x86Ailwkv"
      },
      "source": [
        "sk_params = {\n",
        "    'batch_size':grid_result.best_params_['ann__batch_size'],\n",
        "    'n_hidden': grid_result.best_params_['ann__n_hidden'],\n",
        "    'n_neurons':grid_result.best_params_['ann__n_neurons']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkl65W4lwkz"
      },
      "source": [
        "sk_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtNqh8sjObGB"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdwxs1zAObGH"
      },
      "source": [
        "keras_cs = keras.wrappers.scikit_learn.KerasRegressor(build_fn=build_model, epochs=20 , verbose=0, **sk_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89_ILEVbObGL"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('ann', keras_cs)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A_DFELvObGN"
      },
      "source": [
        "model.fit(X_train, y_train);\n",
        "y_fit = model.predict(X_test);\n",
        "print(mean_squared_error(y_test,y_fit))\n",
        "r2_score(y_test, y_fit )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2SdsNjaK0V"
      },
      "source": [
        "Comparemos ahora el desempeño de nuestro red neuronal modelo con un regresion lineal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuSjMd_aaVeZ"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGeM20LDavWh"
      },
      "source": [
        "model =  Pipeline([\n",
        "                   ('scale', StandardScaler()), ('linear_model', LinearRegression())\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DF52Cxea5n0"
      },
      "source": [
        "model.fit(X_train,y_train)\n",
        "y_fit =  model.predict(X_test)\n",
        "print(mean_squared_error(y_test,y_fit))\n",
        "r2_score(y_test,y_fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFb2J1Cytzb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekDzAhgkL-b5"
      },
      "source": [
        "#Ejercicio Clasificación biclase\n",
        "En éste ejercicio usaremos una red densa para hacer una clasificación biclase para el sentimiento de la reseña de películas de la base de datos IMDB, con label 0 (negativo) y 1 (positivo).\n",
        "\n",
        "En el dataset se encuentran una reseña en texto para una película así como su sentimiento, la idea es construír un clasificador capaz de diferenciar reseñas positivas de negativas:\n",
        "\n",
        "Bebe construír su modelo de la siguiente manera\n",
        "\n",
        "\n",
        "1.   Importe las librerías necesarias (ya lo hemos hecho )\n",
        "2.   Importe los datos de la base de datos del IMDB disponibles en Kereas (También lo hemos hecho.)\n",
        "3.   Cree una red con tres capas: keras.layers.Embedding, ésta capa se encarga de codificar el texto (como el Tf-iDF en el curso 1).\n",
        "  keras.layers.GlobalAveragePooling1D, ésta capa se encarga de promediar los valores de los vectores envevidos en la capa anterior para extraer la información relevante, y keras.layers.Dense que constará de una sola neurona (0 o 1) y una activación sigmoidal para la clasificación biclase.\n",
        "4.  Compile el modelo usando entropía cruzada binaria como perdida y un optimizador 'adam', el cual es una forma modificada del SDG (optimizer='adam',loss='binary_crossentropy'), use 10 épocas\n",
        "5.   Entrene su modelo y evalue la perdida y el accuracy. No olvide que los datos deben ser particionados en datos de entrenamiento y prueba.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZGtf9CXMHCm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSWWqMxMS4_"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO-LJfk-MZOo"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install -q tensorflow-datasets\n",
        "!pip install -q seaborn\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIw7UGMjNHr2"
      },
      "source": [
        "#cargue la base de datos \n",
        "\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews/subwords8k', \n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    as_supervised=True,\n",
        "    with_info=True)\n",
        ")\n",
        "\n",
        "\n",
        "#prepare los datos para entrenar una red densa\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = (\n",
        "    train_data\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .padded_batch(32, train_data.output_shapes))\n",
        "\n",
        "test_batches = (\n",
        "    test_data\n",
        "    .padded_batch(32, train_data.output_shapes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVeyJ83yQQ7M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM6PlHoHQlH2"
      },
      "source": [
        "# Ejercicio Clasificación multiclase\n",
        "\n",
        "En éste caso usaremos la base de datos CIFAR10, la cual consta de imagenes a color, constituidas por 10 clases de objeto: ['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks']. Construirémos de nuevo una red densa para crear un clasificador de imagenes. \n",
        "Recuerde que la idea es generar un flujo de trabajo desde la carga de los datos hasta la evaluación del modelo.\n",
        "\n",
        "1.  Realice la carga de los datos del CIFAR10 de Keras y normalice los datos. Recuerde que al ser imagenes los valores de los pixeles estarán entre 0 y 255.\n",
        "2.  Visualice algunas imagenes de muestra y su label respectivo. Recuerde usar la función imshow del matplotlib.pyplot.\n",
        "3.  Cree un modelo con 4 capas, keras.layers.Flatten, ésta es la capa de entrada y deberá recibir imagenes de 32x32 pixeles y 3 canales de color. Añada 2 capas densas con activación relu y 256 y 128 neuronas respectivamente, finalmente una capa densa de 10 neuronas (el numero de clases del dataset) y con una activación softmax encargada de distribuir las densidad de probabilidad entre todos los labels\n",
        "4.  Recuerde compilar el modelo antes de entrenar. Hágalo con una sparse_categorical_crossentropy como función de perdida y de nuevo un optimizador adam.\n",
        "5. Entrene y evalúe el modelo (recuerde usar datos diferentes para cada una de éstas tareas).\n",
        "6.  Finalmente grafique de nuevo las imagenes como en el punto 2. pero ahora con la etiqueta determinada por la red.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUB65PfZAyKm"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4TKz6V5RyHh"
      },
      "source": [
        "#cargar el dataset\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "#normalice los datos\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfc9JI6LA2i0"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-sCMZfQA-S-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiumnadxBCcn"
      },
      "source": [
        "plt.imshow(train_images[50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1bqfZV-BK5a"
      },
      "source": [
        "train_labels[50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thIYBGyTRqmV"
      },
      "source": [
        "#@title Solución\n",
        "#Visualice una muestra\n",
        "class_names = ['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks']\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS2uOHfDBdco"
      },
      "source": [
        "#construya el modelo\n",
        "#32x32x3 son las dimensiones de las imagenes que serán puestas en un solo vector por ésta capa para poder ser usadas en la capa densa.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(32, 32, 3)), \n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "    #note que ha cambiado la función de activación en la última capa, no en las intermedias\n",
        "\n",
        "#compile el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ICRcq9Bzy1"
      },
      "source": [
        "#entrene el modelo\n",
        "model.fit(train_images, train_labels, epochs=10)\n",
        "\n",
        "#evaluelo\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "#prediga\n",
        "predictions = model.predict(test_images)\n",
        "#grafique\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i])\n",
        "    plt.xlabel(class_names[test_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIE3E5L8U9PB"
      },
      "source": [
        "Note que las capas críticas en la red son:\n",
        "las primeras encargadas de codificar los datos de una manera adecuada para la red. \n",
        "En el caso del texto usamos una red que codifica las palabras como vectores numéricos (igual que el caso del curso 1) y en las imagenes usamos una capa que aplana las imagenes como un vector para ingresarlo a la redy las capas finales que se diferencian por la función de activación y el numero de neuronas en la capa, ésto debe ser representativo del problema que  queremos solucionar.\n",
        "\n",
        "Las capas intermedias son las encargadas de extraer la información de nuestros datos y la importancia de su variedad será evidente en ejercicios posteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BdZFdlNToU6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}